
Lmod is automatically replacing "gcc/14.2.0" with "gcc-native/12.3".

AWS OFI NCCL Plugin v1.6.0 loaded for CUDA 12.6.1
This provides network transport ONLY - NCCL library from PyTorch
Configured for Slingshot11/CXI interconnect
CUDA 12.6.1 loaded with compatible AWS OFI NCCL plugin v1.6.0
Plugin provides network transport - NCCL library from PyTorch
Use with python/miniforge3_pytorch/2.7.0 or equivalent for best compatibility

Activating Modules:
  1) cray-libsci/24.07.0     2) cray-mpich/8.1.30


Lmod is automatically replacing "gcc-native/12.3" with "gcc/14.2.0".


Inactive Modules:
  1) cray-libsci     2) cray-mpich

AWS OFI NCCL Plugin v1.14.2 loaded (Cray PE 25.5)
This provides network transport ONLY - NCCL library from PyTorch
Configured for Slingshot11/CXI interconnect
Built with: libfabric 2.2.0rc1, CUDA 12.9
Lmod has detected the following error: The following module(s) are unknown:
"libfabric/2.2.0rc1"

Please check the spelling or version number. Also try "module spider ..."
It is also possible your cache file is out-of-date; it may help to try:
  $ module --ignore_cache load "libfabric/2.2.0rc1"

Also make sure that all modulefiles written in TCL start with the string
#%Module

Executing this command requires loading "libfabric/2.2.0rc1" which failed while
processing the following module(s):

    Module fullname                 Module Filename
    ---------------                 ---------------
    nccl-ofi-plugin/1.14.2-cuda129  /sw/user/modules/nccl-ofi-plugin/1.14.2-cuda129.lua


[2026-02-05 01:14:24] WARNING - RUN - run.py: main - 217: --reuse is not set, will not reuse previous (before one day) temporary files
Traceback (most recent call last):
  File "/work/nvme/bdqf/william/jiaqi/CoVT/VLMEvalKit/run.py", line 501, in <module>
    load_env()
  File "/work/nvme/bdqf/william/jiaqi/CoVT/VLMEvalKit/run.py", line 250, in main
TypeError: 'NoneType' object is not iterable
E0205 01:14:29.392000 972768 /work/nvme/bdqf/william/.conda/envs/vlmeval/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py:984] failed (exitcode: 1) local_rank: 0 (pid: 973555) of binary: /u/yli8/.conda/envs/vlmeval/bin/python3.10
Traceback (most recent call last):
  File "/u/yli8/.conda/envs/vlmeval/bin/torchrun", line 6, in <module>
    sys.exit(main())
  File "/u/yli8/.conda/envs/vlmeval/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 362, in wrapper
    return f(*args, **kwargs)
  File "/u/yli8/.conda/envs/vlmeval/lib/python3.10/site-packages/torch/distributed/run.py", line 991, in main
    run(args)
  File "/u/yli8/.conda/envs/vlmeval/lib/python3.10/site-packages/torch/distributed/run.py", line 982, in run
    elastic_launch(
  File "/u/yli8/.conda/envs/vlmeval/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 170, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/u/yli8/.conda/envs/vlmeval/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 317, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
run.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2026-02-05_01:14:29
  host      : gh066.hsn.cm.delta.internal.ncsa.edu
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 973555)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
/var/spool/slurmd/job1869382/slurm_script: line 32: --model: command not found
